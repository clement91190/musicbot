{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import cPickle\n",
      "\n",
      "\n",
      "def plot_list_from_file(path='plot_cost_music.pkl', title='Training RMSE'):\n",
      "    with open(path) as f:\n",
      "        y = cPickle.load(f)\n",
      "    x = range(len(y))\n",
      "    \n",
      "    \n",
      "    print y\n",
      "    print x\n",
      "    plt.plot(x, y)\n",
      "    plt.title(title)\n",
      "    plt.ylabel('rmse')\n",
      "    plt.xlabel('HF steps')\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_list_from_file()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[64.296326465606683, 29.428858404159545, 45.138836903572084, 45.984252724647519, 0.69922293245792388, 0.66248826642831171, 0.60479320218165711, 0.51864859451850254, 0.40947427521149315, 0.29017317866285641, 0.21333503685891628, 0.16887251537293196, 0.14676165057967105, 0.13855020257954795, 0.12471835968395074, 0.1192687657661736, 0.11029695549358924, 0.10035682666425903, 0.089401011991625035, 0.077369746115679539, 0.068683422909428679, 0.06251543020519118, 0.059881381941959265, 0.056222993914658827, 0.05015888081397861, 0.047747290777042511, 0.049421560247428713, 0.045401817223367592, 0.04438771443984782, 0.03884940831689164, 0.041267377212643626, 0.040828377621558803, 0.038905610460011911, 0.039213765861156089, 0.038050879824052877, 0.037060565610881897, 0.037103414664355419, 0.036089970287866889, 0.036960283475151907, 0.037814983722443379, 0.034269302194006743, 0.036300007048218202, 0.038853813725290821, 0.037592191943743573, 0.037316095772354556, 0.036270537927436335, 0.036138016059218597, 0.034588984595611688, 0.039985335158416999, 0.035851415916501234, 0.037394020236097274, 0.035388978460493187, 0.036980705898410328, 0.0365605572774075, 0.035454917338987195, 0.03648806241340935, 0.72975093603134156]\n",
        "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_list_from_file( 'test_errors.pkl', title='Testing error')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[549.69629, 341.64719, 256.1897, 225.50368, 105.93655, 129.81454, 298.78839, 432.67538, 261.45718, 230.39183]\n",
        "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Experiment with the weight matrix, previously trained to remember the first input of a binary 100 timesteps random sequences. \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle\n",
      "import networkx as nx\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "with open('data/weight_matrix.pkl') as f:\n",
      "    whh = cPickle.load(f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "whh.shape\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "(100, 100)"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "graph = nx.Graph(np.array(whh, dtype = [('weight', float)]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pos=nx.spectral_layout(graph)\n",
      "#pos=nx.spectral_layout(graph)\n",
      "#print pos\n",
      "elarge=[(u,v) for (u,v,d) in graph.edges(data=True) if d['weight'] > 0.15]\n",
      "esmall=[(u,v) for (u,v,d) in graph.edges(data=True) if d['weight'] < -0.15]\n",
      "nx.draw_networkx_edges(graph, pos, edgelist=elarge, width=0.1, edge_color='b')\n",
      "nx.draw_networkx_edges(graph, pos, edgelist=esmall, width=0.1, )\n",
      "nx.draw_networkx_nodes(graph, pos, node_size=10)\n",
      "nx.draw_networkx_labels(graph, pos)\n",
      "plt.axis('off')\n",
      "plt.savefig(\"weighted_graph_spring.png\", dpi=100) # save as png\n",
      "plt.show() # display"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.mean(whh)\n",
      "print np.max(whh)\n",
      "print np.min(whh)\n",
      "whh_abs = abs( whh)\n",
      "print np.mean(whh_abs)\n",
      "print np.min(whh_abs)\n",
      "\n",
      "def plot_weight_distribution(w, n=100):\n",
      "    \n",
      "    w_max = np.max(w)\n",
      "    w_min = np.min(w)\n",
      "    \n",
      "    #inter = np.logspace(np.log(w_min), np.log(w_max), n)\n",
      "    inter = np.linspace(w_min, w_max, n)\n",
      "    print inter\n",
      "    eps = (w_max - w_min) / n\n",
      "    weight_list = [ 0 for i in range(len(inter))]\n",
      "    for i in w:\n",
      "        for j in i:\n",
      "            val = j\n",
      "            for ind, v in enumerate(inter[:-1]):\n",
      "                if v <= val < inter[ind + 1]:\n",
      "                    weight_list[ind] += 1 / ( inter[ind + 1] - v) \n",
      "    return weight_list / np.sum(weight_list), inter\n",
      "            \n",
      "w_list, intervals = plot_weight_distribution(whh_abs)\n",
      "print w_list\n",
      "plt.plot(intervals, w_list)\n",
      "plt.title('Distribution of the absolute value of the weights')\n",
      "plt.ylabel('')\n",
      "plt.xlabel('Weight')\n",
      "plt.savefig(\"abs_weight_distribution\", dpi=100)\n",
      "w_list, intervals = plot_weight_distribution(whh)\n",
      "#print w_list\n",
      "#plt.plot(intervals, w_list)\n",
      "#plt.title('Distribution of the weights')\n",
      "#plt.ylabel('')\n",
      "#plt.xlabel('Weight')\n",
      "#plt.savefig(\"weight_distribution\", dpi=100)\n",
      "\n",
      "plt.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-0.00100584\n",
        "0.234827\n",
        "-0.234201\n",
        "0.0999132\n",
        "4.00321e-05\n",
        "[  4.00320860e-05   2.41161879e-03   4.78320550e-03   7.15479221e-03\n",
        "   9.52637892e-03   1.18979656e-02   1.42695523e-02   1.66411390e-02\n",
        "   1.90127257e-02   2.13843125e-02   2.37558992e-02   2.61274859e-02\n",
        "   2.84990726e-02   3.08706593e-02   3.32422460e-02   3.56138327e-02\n",
        "   3.79854194e-02   4.03570061e-02   4.27285928e-02   4.51001795e-02\n",
        "   4.74717662e-02   4.98433529e-02   5.22149397e-02   5.45865264e-02\n",
        "   5.69581131e-02   5.93296998e-02   6.17012865e-02   6.40728732e-02\n",
        "   6.64444599e-02   6.88160466e-02   7.11876333e-02   7.35592200e-02\n",
        "   7.59308067e-02   7.83023934e-02   8.06739801e-02   8.30455668e-02\n",
        "   8.54171536e-02   8.77887403e-02   9.01603270e-02   9.25319137e-02\n",
        "   9.49035004e-02   9.72750871e-02   9.96466738e-02   1.02018261e-01\n",
        "   1.04389847e-01   1.06761434e-01   1.09133021e-01   1.11504607e-01\n",
        "   1.13876194e-01   1.16247781e-01   1.18619367e-01   1.20990954e-01\n",
        "   1.23362541e-01   1.25734128e-01   1.28105714e-01   1.30477301e-01\n",
        "   1.32848888e-01   1.35220474e-01   1.37592061e-01   1.39963648e-01\n",
        "   1.42335235e-01   1.44706821e-01   1.47078408e-01   1.49449995e-01\n",
        "   1.51821581e-01   1.54193168e-01   1.56564755e-01   1.58936341e-01\n",
        "   1.61307928e-01   1.63679515e-01   1.66051102e-01   1.68422688e-01\n",
        "   1.70794275e-01   1.73165862e-01   1.75537448e-01   1.77909035e-01\n",
        "   1.80280622e-01   1.82652209e-01   1.85023795e-01   1.87395382e-01\n",
        "   1.89766969e-01   1.92138555e-01   1.94510142e-01   1.96881729e-01\n",
        "   1.99253316e-01   2.01624902e-01   2.03996489e-01   2.06368076e-01\n",
        "   2.08739662e-01   2.11111249e-01   2.13482836e-01   2.15854422e-01\n",
        "   2.18226009e-01   2.20597596e-01   2.22969183e-01   2.25340769e-01\n",
        "   2.27712356e-01   2.30083943e-01   2.32455529e-01   2.34827116e-01]\n",
        "[ 0.01130113  0.01230123  0.01210121  0.01340134  0.01270127  0.01150115\n",
        "  0.01260126  0.01160116  0.01260126  0.00990099  0.01330133  0.01140114\n",
        "  0.01160116  0.01120112  0.01220122  0.01090109  0.01250125  0.01450145\n",
        "  0.01290129  0.01150115  0.01430143  0.01230123  0.0120012   0.01270127\n",
        "  0.0110011   0.01170117  0.01390139  0.01040104  0.01010101  0.01310131\n",
        "  0.01120112  0.01050105  0.01170117  0.01090109  0.01050105  0.01280128\n",
        "  0.01310131  0.01210121  0.01060106  0.01190119  0.01240124  0.00940094\n",
        "  0.01160116  0.01120112  0.01150115  0.010001    0.01240124  0.01190119\n",
        "  0.01320132  0.01130113  0.01250125  0.01140114  0.01120112  0.01360136\n",
        "  0.01210121  0.01040104  0.01410141  0.01210121  0.01070107  0.0120012\n",
        "  0.01120112  0.01210121  0.0110011   0.01150115  0.0130013   0.01070107\n",
        "  0.01330133  0.01280128  0.01110111  0.01110111  0.01150115  0.01080108\n",
        "  0.01270127  0.01160116  0.00990099  0.01240124  0.00990099  0.01080108\n",
        "  0.01040104  0.00880088  0.00980098  0.00790079  0.00780078  0.00750075\n",
        "  0.00580058  0.00510051  0.00310031  0.00320032  0.00210021  0.00190019\n",
        "  0.00180018  0.00090009  0.00080008  0.00040004  0.00050005  0.00050005\n",
        "  0.00010001  0.00010001  0.00020002  0.        ]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[-0.23420143 -0.22946377 -0.22472611 -0.21998844 -0.21525078 -0.21051312\n",
        " -0.20577546 -0.2010378  -0.19630013 -0.19156247 -0.18682481 -0.18208715\n",
        " -0.17734949 -0.17261182 -0.16787416 -0.1631365  -0.15839884 -0.15366118\n",
        " -0.14892351 -0.14418585 -0.13944819 -0.13471053 -0.12997287 -0.1252352\n",
        " -0.12049754 -0.11575988 -0.11102222 -0.10628455 -0.10154689 -0.09680923\n",
        " -0.09207157 -0.08733391 -0.08259624 -0.07785858 -0.07312092 -0.06838326\n",
        " -0.0636456  -0.05890793 -0.05417027 -0.04943261 -0.04469495 -0.03995729\n",
        " -0.03521962 -0.03048196 -0.0257443  -0.02100664 -0.01626897 -0.01153131\n",
        " -0.00679365 -0.00205599  0.00268167  0.00741934  0.012157    0.01689466\n",
        "  0.02163232  0.02636998  0.03110765  0.03584531  0.04058297  0.04532063\n",
        "  0.05005829  0.05479596  0.05953362  0.06427128  0.06900894  0.0737466\n",
        "  0.07848427  0.08322193  0.08795959  0.09269725  0.09743492  0.10217258\n",
        "  0.10691024  0.1116479   0.11638556  0.12112323  0.12586089  0.13059855\n",
        "  0.13533621  0.14007387  0.14481154  0.1495492   0.15428686  0.15902452\n",
        "  0.16376218  0.16849985  0.17323751  0.17797517  0.18271283  0.1874505\n",
        "  0.19218816  0.19692582  0.20166348  0.20640114  0.21113881  0.21587647\n",
        "  0.22061413  0.22535179  0.23008945  0.23482712]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#input weight sum (hidden weights)\n",
      "in_w = np.sum(whh_abs, 0)\n",
      "#output weight sum (hidden weights)\n",
      "out_w = np.sum(whh_abs, 1)\n",
      "def plot_weight_sum_distribution(w, n=20):\n",
      "    \n",
      "    w_max = np.max(w)\n",
      "    w_min = np.min(w)\n",
      "    \n",
      "    #inter = np.logspace(np.log(w_min), np.log(w_max), n)\n",
      "    inter = np.linspace(w_min, w_max, n)\n",
      "    print inter\n",
      "    eps = (w_max - w_min) / n\n",
      "    weight_list = [ 0 for i in range(len(inter))]\n",
      "    for j in w:\n",
      "        val = j\n",
      "        for ind, v in enumerate(inter[:-1]):\n",
      "            if v <= val < inter[ind + 1]:\n",
      "                weight_list[ind] += 1 / ( inter[ind + 1] - v) \n",
      "    return weight_list / np.sum(weight_list), inter\n",
      "w_list, intervals = plot_weight_sum_distribution(out_w)\n",
      "#print w_list\n",
      "plt.plot(intervals, w_list)\n",
      "plt.title('Distribution of the sum of the output connection weights on Whh ')\n",
      "plt.ylabel('')\n",
      "plt.xlabel('Weight sum')\n",
      "plt.savefig(\"output_sum_weight_distribution\", dpi=100)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[  9.01629066   9.16544969   9.31460872   9.46376775   9.61292678\n",
        "   9.76208581   9.91124484  10.06040387  10.2095629   10.35872193\n",
        "  10.50788096  10.65703999  10.80619902  10.95535805  11.10451708\n",
        "  11.25367611  11.40283514  11.55199417  11.7011532   11.85031223]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum_abs_weight =sorted([(in_w[i] + out_w[i], i) for i in range(len(in_w))])\n",
      "#input weight sum (hidden weights)\n",
      "in_w_2 = np.sum(whh, 0)\n",
      "#output weight sum (hidden weights)\n",
      "out_w_2 = np.sum(whh, 1)\n",
      "sum_weight =sorted([(in_w_2[i] + out_w_2[i], i) for i in range(len(in_w_2))])\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#loading some code for testing a RNN on the binary memory exercise\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "import numpy\n",
      "\n",
      "class SequenceDataset:\n",
      "    '''Slices, shuffles and manages a small dataset for the HF optimizer.'''\n",
      "    \n",
      "    def __init__(self, data, batch_size, number_batches, minimum_size=10):\n",
      "        '''SequenceDataset __init__\n",
      "        \n",
      "        data : list of lists of numpy arrays\n",
      "        Your dataset will be provided as a list (one list for each graph input) of\n",
      "        variable-length tensors that will be used as mini-batches. Typically, each\n",
      "        tensor is a sequence or a set of examples.\n",
      "        batch_size : int or None\n",
      "        If an int, the mini-batches will be further split in chunks of length\n",
      "        `batch_size`. This is useful for slicing subsequences or provide the full\n",
      "        dataset in a single tensor to be split here. All tensors in `data` must\n",
      "        then have the same leading dimension.\n",
      "        number_batches : int\n",
      "        Number of mini-batches over which you iterate to compute a gradient or\n",
      "        Gauss-Newton matrix product.\n",
      "        minimum_size : int\n",
      "        Reject all mini-batches that end up smaller than this length.'''\n",
      "        self.current_batch = 0\n",
      "        self.number_batches = number_batches\n",
      "        self.items = []\n",
      "        \n",
      "        for i_sequence in xrange(len(data[0])):\n",
      "            if batch_size is None:\n",
      "                self.items.append([data[i][i_sequence] for i in xrange(len(data))])\n",
      "            else:\n",
      "                for i_step in xrange(0, len(data[0][i_sequence]) - minimum_size + 1, batch_size):\n",
      "                    self.items.append([data[i][i_sequence][i_step:i_step + batch_size] for i in xrange(len(data))])\n",
      "              \n",
      "        self.shuffle()\n",
      "    \n",
      "    def shuffle(self):\n",
      "        numpy.random.shuffle(self.items)\n",
      "    \n",
      "    def iterate(self, update=True):\n",
      "        for b in xrange(self.number_batches):\n",
      "            yield self.items[(self.current_batch + b) % len(self.items)]\n",
      "        if update: self.update()\n",
      "    \n",
      "    def update(self):\n",
      "        if self.current_batch + self.number_batches >= len(self.items):\n",
      "            self.shuffle()\n",
      "            self.current_batch = 0\n",
      "        else:\n",
      "            self.current_batch += self.number_batches\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "# single-layer recurrent neural network with sigmoid output, only last time-step output is significant\n",
      "def simple_RNN(nh , p=None):\n",
      "    if p is not None:\n",
      "        Wx = theano.shared(p[0])\n",
      "        Wh = theano.shared(p[1])\n",
      "        Wy = theano.shared(p[2])\n",
      "        bh = theano.shared(p[3])\n",
      "        by = theano.shared(p[4])\n",
      "        h0 = theano.shared(p[5])\n",
      "    else:\n",
      "        Wx = theano.shared(0.2 * numpy.random.uniform(-1.0, 1.0, (1, nh)).astype(theano.config.floatX))\n",
      "        Wh = theano.shared(0.2 * numpy.random.uniform(-1.0, 1.0, (nh, nh)).astype(theano.config.floatX))\n",
      "        Wy = theano.shared(0.2 * numpy.random.uniform(-1.0, 1.0, (nh, 1)).astype(theano.config.floatX))\n",
      "        bh = theano.shared(numpy.zeros(nh, dtype=theano.config.floatX))\n",
      "        by = theano.shared(numpy.zeros(1, dtype=theano.config.floatX))\n",
      "        h0 = theano.shared(numpy.zeros(nh, dtype=theano.config.floatX))\n",
      "    \n",
      "    p = [Wx, Wh, Wy, bh, by, h0]\n",
      "\n",
      "    x = T.matrix()\n",
      "\n",
      "    def recurrence(x_t, h_tm1):\n",
      "        ha_t = T.dot(x_t, Wx) + T.dot(h_tm1, Wh) + bh\n",
      "        h_t = T.tanh(ha_t)\n",
      "        s_t = T.dot(h_t, Wy) + by\n",
      "        return [ha_t, h_t, s_t]\n",
      "\n",
      "    ([ha, h, activations], updates) = theano.scan(fn=recurrence, sequences=x, outputs_info=[dict(), h0, dict()])\n",
      "\n",
      "    h = T.tanh(ha)  # so it is differentiable with respect to ha\n",
      "    t = x[0, 0]\n",
      "    s = activations[-1, 0]\n",
      "    y = T.nnet.sigmoid(s)\n",
      "    loss = -t*T.log(y + 1e-14) - (1-t)*T.log((1-y) + 1e-14)\n",
      "    acc = T.neq(T.round(y), t)\n",
      "  \n",
      "    return p, [x], s, [loss, acc], h, ha\n",
      "\n",
      "\n",
      "def test_RNN(p):\n",
      "    p, inputs, s, costs, h, ha = simple_RNN(100, p)\n",
      "    memorization_dataset = [[]]  # memorize the first unit for 100 time-steps with binary noise\n",
      "    for i in xrange(10000):\n",
      "        memorization_dataset[0].append(numpy.random.randint(2, size=(100, 1)).astype(theano.config.floatX))\n",
      "\n",
      "    valid = [memorization_dataset[0]]\n",
      "    valid_dataset = SequenceDataset(valid, batch_size=None, number_batches=1000)\n",
      "    \n",
      "    f_cost = theano.function(inputs, costs, on_unused_input='ignore')  # for quick cost evaluation\n",
      "    res = numpy.mean([f_cost(*i) for i in valid_dataset.iterate()], axis=0)\n",
      "    \n",
      "    return res\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#loading all the parameters of the network\n",
      "import cPickle\n",
      "\n",
      "\n",
      "with open('parameters_net.pkl') as f:\n",
      "    params = cPickle.load(f)\n",
      "\n",
      "\n",
      "def test_l(l=None):\n",
      "    if l is None:\n",
      "        n_neuron_to_erase = 2\n",
      "        l = random.sample(range(whh.shape[0]), n_neuron_to_erase)\n",
      "         \n",
      "    param_copy = [numpy.array(i) for i in params]\n",
      "    #modification of the network here\n",
      "    whh = param_copy[1]\n",
      "    import random\n",
      "    \n",
      "    def brain_deletion(w, l):   \n",
      "        for i in l:\n",
      "            #delete row\n",
      "            w[i] = 0\n",
      "            #delete column\n",
      "            w[:, i] = 0\n",
      "       \n",
      "    #print \"deleting neurons\" , l\n",
      "    brain_deletion(whh, l)    \n",
      "    \n",
      "    #testing the RNN\n",
      "        \n",
      "    return test_RNN(param_copy)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = [ test_l([i])[1] for i in range(100) ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/theano/scan_module/scan_perform_ext.py:117: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n",
        "  from scan_perform.scan_perform import *\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "n = sorted([(r, i) for i, r in enumerate(res)])\n",
      "score = [i[0] for i in n]\n",
      "order = [i[1] for i in n]\n",
      "ordered = sorted(sum_abs_weight, key = lambda (x,y) :y)\n",
      "score_2 = [ordered[i[1]][0] for i in n]\n",
      "\n",
      "v = max(score_2)\n",
      "score_2 = [i/v for i in score_2] \n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def hamming_dist(a, b):\n",
      "    return sum([abs(i -j) for i, j in zip(a, b)])\n",
      "    \n",
      "\n",
      "def results_ordering(scores):\n",
      "    ordered = sorted(scores, key = lambda (x,y) :y)\n",
      "    score_2 = [ordered[i[1]][0] for i in n]\n",
      "    ordered_score = sorted(scores, key = lambda (x,y) :x)\n",
      "    permut = [i[1] for i in ordered_score]\n",
      "    \n",
      "    \n",
      "    v = max(score_2)\n",
      "    score_2 = [i/v for i in score_2] \n",
      "\n",
      "    \n",
      "    return score_2 , hamming_dist(permut, order)\n",
      "    \n",
      "    \n",
      "\n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#study of the metric using sum of abs weight\n",
      "\n",
      "score_2, d = results_ordering(sum_abs_weight)\n",
      "print \"hamming distance\" , d\n",
      "plt.plot(range(len(score)), score, range(len(score)), score_2)\n",
      "#plt.plot(range(len(score_2)), score_2)\n",
      "plt.title('M1 value for the sorted list of neuron')\n",
      "plt.ylabel('Error percentage')\n",
      "plt.xlabel('indice in the sorted list')\n",
      "plt.savefig(\"m1\", dpi=100) # save as png\n",
      "plt.show() # displayplt.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hamming distance 3118\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#study of the metric using sum of weight\n",
      "\n",
      "score_2, d = results_ordering(sum_weight)\n",
      "print \"hamming distance\" , d\n",
      "plt.plot(range(len(score)), score, range(len(score)), score_2)\n",
      "#plt.plot(range(len(score_2)), score_2)\n",
      "plt.title('M2 value for the sorted list of neuron')\n",
      "plt.ylabel('Error percentage')\n",
      "plt.xlabel('indice in the sorted list')\n",
      "plt.savefig(\"m2\", dpi=100) # save as png\n",
      "plt.show() # displayplt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hamming distance 3294\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#study of the metric using sum of weight\n",
      "\n",
      "score_2, d = results_ordering([(abs(i[0]), i[1]) for i in sum_weight])\n",
      "print \"hamming distance\" , d\n",
      "plt.plot(range(len(score)), score, range(len(score)), score_2)\n",
      "#plt.plot(range(len(score_2)), score_2)\n",
      "plt.title('M3 value for the sorted list of neuron')\n",
      "plt.ylabel('Error percentage')\n",
      "plt.xlabel('indice in the sorted list')\n",
      "plt.savefig(\"m3\", dpi=100) # save as png\n",
      "\n",
      "plt.show() # displayplt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hamming distance 3512\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#clustetring index\n",
      "\n",
      "graph = nx.Graph(np.array(abs(whh + whh.T), dtype = [('weight', float)]))\n",
      "cluster_coef = nx.algorithms.cluster.clustering(graph, weight='weight')\n",
      "score_3 = [ (j, i) for i, j in cluster_coef.iteritems()]\n",
      "score_3, d = results_ordering([(abs(i[0]), i[1]) for i in score_3])\n",
      "print \"hamming distance\" , d\n",
      "plt.plot(range(len(score)), score, range(len(score)), score_3)\n",
      "#plt.plot(range(len(score_2)), score_2)\n",
      "plt.title('M4 value for the sorted list of neuron')\n",
      "plt.ylabel('Error percentage')\n",
      "plt.xlabel('indice in the sorted list')\n",
      "plt.savefig(\"m4\", dpi=100) # save as png\n",
      "plt.show() # displayplt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hamming distance 3388\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# abs of output coef\n",
      "score_4 = zip([i[0] for i in abs(params[2])], range(len(score)))\n",
      "score_4, d = results_ordering([(abs(i[0]), i[1]) for i in score_4])\n",
      "print \"hamming distance\" , d\n",
      "plt.plot(range(len(score)), score, range(len(score)), score_4)\n",
      "#plt.plot(range(len(score_2)), score_2)\n",
      "plt.title('M5 value for the sorted list of neuron')\n",
      "plt.ylabel('Error percentage')\n",
      "plt.xlabel('indice in the sorted list')\n",
      "plt.savefig(\"m5\", dpi=100) # save as png\n",
      "plt.show() # displayplt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hamming distance 3244\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#abs of input coef\n",
      "score_5 = zip([i for i in abs(params[0])[0]], range(len(score)))\n",
      "score_5, d = results_ordering([(abs(i[0]), i[1]) for i in score_5])\n",
      "print \"hamming distance\" , d\n",
      "plt.plot(range(len(score)), score, range(len(score)), score_5)\n",
      "#plt.plot(range(len(score_2)), score_2)\n",
      "plt.title('M6 value for the sorted list of neuron')\n",
      "plt.ylabel('Error percentage')\n",
      "plt.xlabel('indice in the sorted list')\n",
      "plt.savefig(\"m6\", dpi=100) # save as png\n",
      "plt.show() # displayplt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hamming distance 2944\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sum of input and output \n",
      "score_5 = [i for i in abs(params[0])[0]]\n",
      "score_4 = [i[0] for i in abs(params[2])]\n",
      "score_6 = zip ( [i + j for i, j in zip(score_4, score_5) ], range(len(score)))\n",
      "score_6, d = results_ordering([(abs(i[0]), i[1]) for i in score_6])\n",
      "print \"hamming distance\" , d\n",
      "plt.plot(range(len(score)), score, range(len(score)), score_6)\n",
      "#plt.plot(range(len(score_2)), score_2)\n",
      "plt.title('M7 value for the sorted list of neuron')\n",
      "plt.ylabel('Error percentage')\n",
      "plt.xlabel('indice in the sorted list')\n",
      "plt.savefig(\"m7\", dpi=100) # save as png\n",
      "plt.show() # displayplt.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hamming distance 3402\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}