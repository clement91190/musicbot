% Chapter Template

\chapter{Introduction} % Main chapter title

\label{Chapter 1} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter 1. \emph{Related Work}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

%\section{State of the Art}

\section{Motivation and Aims}
Recurrent Neural Networks have won several awards in the last decade to model complex sequences. Many ameliorations in the learning algorithm as well as the growth of computational power makes it possible to train these models today, whereas it seemed impossible 20 years ago. The reason for the difficulty of training a Recurrent Neural Network (RNN) compared to a Feed Forward Neural Network (FFNN) is that the backpropagation algorithm used for both makes the RNN equivalent to a very deep (as deep as the timestep of the sequence) FFNN. Many of the techniques used with RNN are therefore modified from their significant sucess on FFNN, however specific work on the recurrent structure of the RNN is needed to understand its complex dynamics. Long Short Term Memory RNNs (LSTM \cite{hochreiter1997long}) are a specific modification of this structure that proved to be very efficient. Understanding and introducing tools to grasp the complexity of a RNN can also be useful in Neuroscience: if we have precise models of the functioning of a neurons in the brain, the brain interactions are still a wide subject of research.   

In this project we aim to study the internal structure of RNNs and propose improvements in the learning and design of the network as well as use a MIDI database to train such a network. 

\section{Contributions}
    This project is divided in two parts. First we trained a Recurrent Neural Network on a specific problem and studied its internal structure. This led to empirical results that confirm the lack of understanding of the dynamics that take place inside a Recurrent Neural Network. Different Graph theory techniques were used on the graph representing the Recurrent Neural Network. In the second part of the project we introduce a new learning algorithm for training Recurrent Neural Network that optimizes both the architecture and the weight of the Network during training. The modification of the architectures are introduced based empirically on the results of the first part. 

